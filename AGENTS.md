# LLM Linguistic Classification Benchmark Agent

## Agent Overview
This agent is a Python-based benchmarking tool designed to evaluate large language models (LLMs) on a linguistic classification task. Each input consists of a text excerpt containing a specific **node word**, and the agent prompts an LLM (via the OpenAI API) to classify that node word according to a given linguistic criterion (e.g. a syntactic category like part-of-speech, or a semantic category). The agent automates the end-to-end process: loading a dataset of labeled examples, querying the LLM for each example, collecting the model’s predicted label along with an explanation and a self-reported confidence, and finally comparing these predictions against the ground truth labels to measure performance. This provides a reproducible way to benchmark an LLM’s accuracy on specialized linguistic classification problems.

## Inputs
- **Dataset CSV**: A CSV file containing the evaluation dataset. Each row should include at least:
  - a text excerpt (context sentence or paragraph),
  - the target **node word** within that text (the word to classify),
  - the ground truth label for the node word (the correct category according to the chosen linguistic criterion).
- **Label Schema**: The set of possible labels or categories for classification. This can be derived from the dataset (e.g. all unique labels in the CSV) or provided explicitly. The agent uses this to instruct the LLM on valid label options.
- **OpenAI API Credentials**: Access to the OpenAI API (an API key, configured via environment variable or configuration file). The agent requires this to call the LLM. The OpenAI model ID (e.g. `gpt-3.5-turbo`, `gpt-4`, or a specific engine for completion) may be specified as a parameter.
- **Prompt Template** (built-in): The agent includes a prompt format that it uses for each query. This prompt will contain instructions for classification, the list of label options, and the required format for the LLM’s answer. (Optionally, advanced users can adjust the prompt wording or provide a few example Q&A pairs for few-shot prompting, though the default assumes zero-shot with instructions.)
- **Configuration Parameters** (optional):
  - **Max Samples**: Optionally limit the number of inputs to process (useful for large datasets or rate limiting).
  - **Logprobs Flag**: A toggle to enable retrieval of token log probabilities from the API (if supported by the chosen model), used for confidence analysis.
  - **Output Paths**: File paths or names for saving results (e.g. `results.csv` for predictions, `calibration_plot.png` for the plot). Defaults can be provided in the agent.

## Outputs
- **Predictions CSV**: A CSV file (e.g. `results.csv`) is produced, containing a row for each input example with the following columns:
  - *Excerpt ID or index* (if applicable, or the text excerpt truncated for identification).
  - **Node Word**: the target word from the excerpt.
  - **Ground Truth Label**: the correct label from the dataset.
  - **Predicted Label**: the label predicted by the LLM.
  - **Explanation**: a brief explanation generated by the LLM describing why it chose that label (usually one or two sentences).
  - **Model Confidence**: the self-reported confidence from the LLM’s answer (e.g. "Confidence: 85%"), parsed as a percentage.
  - **Log Probability** (optional): the top log-probability of the predicted label token, if retrieved from the API. This is typically a negative log-likelihood value; higher values (closer to 0) mean higher probability. (If logprobs are not available, this field may be blank or omitted.)
  - **Correctness**: a boolean or binary indicator (`TRUE/FALSE` or `1/0`) indicating whether the predicted label matches the ground truth label.
- **Performance Metrics**: After processing all examples, the agent computes summary metrics:
  - Overall **Accuracy** (fraction of correct predictions).
  - **Macro F1 Score** (unweighted average F1-score across all classes).
  - These metrics are typically printed to console for the user or saved to a small report (e.g. as a JSON or text output). For example: *Accuracy: 78.5%*, *Macro F1: 0.75*.  
- **Calibration Plot** (optional): If confidence collection is enabled, the agent can generate a calibration chart comparing the model’s stated confidence to its actual accuracy. For instance, a reliability diagram (saved as an image file like `confidence_calibration.png`) where the x-axis is binned confidence levels and the y-axis is the observed accuracy for predictions in that bin. This helps visualize if the model is over- or under-confident. (This step is optional and may require matplotlib or similar library to produce the plot.)

## Plan
1. **Load Dataset**: Read the input CSV file using a CSV parser (e.g. Python’s `csv` module or pandas). Store each example’s text, node word, and true label. Also, determine the set of possible labels (e.g. all unique labels in the dataset) if not explicitly provided.
2. **Initialize Output Records**: Prepare a data structure (e.g. a list or DataFrame) to accumulate results for each example. This will later be written to the output CSV.
3. **Iterate Over Examples**: For each example in the dataset:
   1. **Construct Prompt**: Formulate the prompt for the LLM. The prompt should clearly instruct the model about the task...
[Truncated for brevity]
